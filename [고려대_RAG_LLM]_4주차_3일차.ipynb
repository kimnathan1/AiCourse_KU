{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit langchain langchain_experimental langchain-openai langchain-community langchain-text-splitters chromadb python-dotenv pyngrok pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T2cFAs6YpnM",
        "outputId": "95663f8f-2103-4e65-dddb-93b41ec48407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken \"31pHSQ5Ej8bCmgm2JaVih9RW9lg_6Byj5qALS13ejC2k5Wzwm\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTh7h9pGY8ji",
        "outputId": "40d8e33a-78d8-4260-ce16-86627e7d715d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 직접 환경변수 등록 (가장 확실한 방법) 아래에 OPEN AI 키 넣기\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"N3WP2eZzcA\"\n",
        "\n",
        "# 확인\n",
        "print(\"✅ OPENAI_API_KEY 등록 완료:\", os.environ.get(\"OPENAI_API_KEY\")[:10] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65kyvaWHZRcG",
        "outputId": "876e816f-45ec-4aee-c42f-5829f4603a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ OPENAI_API_KEY 등록 완료: sk-proj--7...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1단계\n",
        "%%writefile app.py\n",
        "import os, uuid\n",
        "import streamlit as st\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# 기본 메인 화면 설정 페이지 타이틀 / 화면 타이틀\n",
        "st.set_page_config(page_title = \"RAG 챗봇 만들기 Step 1\")\n",
        "st.title(\"기본 LLM 챗봇 만들기\")\n",
        "\n",
        "# 세션 초기화\n",
        "# 임시 메모리 -> 새로고침 전까지 유지되는 메모리 - 대화 내용이 없으면 빈 리스트!\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state[\"messages\"] = []\n",
        "\n",
        "# LLM 구성\n",
        "# 1. AI의 목적 및 프롬프트\n",
        "# 2. 사용자의 대화를 전달받는 q {q}\n",
        "# 3. 답변을 비워놔서 AI답변을 기다린다.\n",
        "llm = ChatOpenAI(model = 'gpt-4o-mini')\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "너는 친절한 한국어 비서야.\n",
        "사용자 : {q}\n",
        "답변  :\n",
        "\"\"\")\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Streamlit 가장 중요한 요소 중 1개 chat_message - 스트림릿의 채팅 UI요소 뼈대\n",
        "# session_state가 가지고 있는 메모리 -> 채팅UI 출력물을 뿌려줘야 하는데\n",
        "# role 사람인지 챗봇인지 구별 -> content를 채팅 UI에 출력!\n",
        "for m in st.session_state[\"messages\"]:\n",
        "    with st.chat_message(m['role']):\n",
        "        st.markdown(m[\"content\"])\n",
        "\n",
        "# 알맹이 사용자 입력 -> LLM 응답 -> 렌더링(실시간 대화)\n",
        "q = st.chat_input(\"질문을 입력해주세요\") # input 설정\n",
        "if q: #q가 질문이 입력이 되었다면\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(q) # q출력\n",
        "    st.session_state['messages'].append({'role' : 'user', 'content' : q})\n",
        "\n",
        "    with st.chat_message('assistant'):\n",
        "        ans = chain.invoke({\"q\": q})\n",
        "        st.markdown(ans)\n",
        "    st.session_state['messages'].append({'role' : 'assistant', 'content': ans})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlmliAREZRqG",
        "outputId": "357e88d3-c064-4e09-d900-759647845abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2단계 - RAG기반 챗봇 구현\n",
        "%%writefile app.py\n",
        "import os, uuid\n",
        "import streamlit as st\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings # 2단계 추가\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_community.document_loaders import TextLoader # 2단계 추가\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter # 2단계 추가\n",
        "from langchain_community.vectorstores import Chroma # 2단계 추가\n",
        "\n",
        "# 2단계에 필요한 라이브러리 추가\n",
        "\n",
        "# 기본 메인 화면 설정 페이지 타이틀 / 화면 타이틀\n",
        "st.set_page_config(page_title = \"RAG 챗봇 만들기 Step 2\")\n",
        "st.title(\"RAG 챗봇 만들기\")\n",
        "\n",
        "# 세션 초기화\n",
        "# 임시 메모리 -> 새로고침 전까지 유지되는 메모리 - 대화 내용이 없으면 빈 리스트!\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state[\"messages\"] = []\n",
        "\n",
        "# LLM 구성\n",
        "# 1. AI의 목적 및 프롬프트\n",
        "# 2. 사용자의 대화를 전달받는 q {q}\n",
        "# 3. 답변을 비워놔서 AI답변을 기다린다.\n",
        "llm = ChatOpenAI(model = 'gpt-4o-mini')\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "너는 친절한 한국어 비서야.\n",
        "아래 문서 내용을 참고해 질문에 답하세요.\n",
        "문서   : {context}\n",
        "사용자 : {q}\n",
        "답변  :\n",
        "\"\"\")\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# RAG관련 문서 추가 - 수업에서는 하단에 있는 4줄 샘플로 진행\n",
        "# 3) 문서→임베딩→DB (2교시 신규 추가)\n",
        "os.makedirs(\"data\", exist_ok=True)                                     # [추가]\n",
        "doc_path = \"data/정책.md\"                                              # [추가]\n",
        "if not os.path.exists(doc_path):                                       # [추가: 샘플 생성]\n",
        "    with open(doc_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\n",
        "            \"- 디지털 상품은 원칙적으로 환불 불가 (결제 오류·중복 결제 예외)\\n\"\n",
        "            \"- 실물 상품은 수령 7일 이내 교환/환불 가능(미개봉)\\n\"\n",
        "            \"- VIP(GOLD) 고객은 1회 예외 환불 가능 (관리자 승인)\\n\"\n",
        "            \"- 배송 지연 3영업일 초과 시 사과 쿠폰 지급(내부 승인)\\n\"\n",
        "        )\n",
        "\n",
        "# 상단에 4줄 문장을 읽을 준비 완료\n",
        "loader = TextLoader(doc_path, encoding = 'utf-8')\n",
        "docs = loader.load()\n",
        "\n",
        "# RAG관련 문서 정제\n",
        "# 1) 문장을 청크(chunk)해서 나누고, 겹치게 구성\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size = 800, chunk_overlap = 100)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "# 2) 임베딩 - 문장을 숫자화\n",
        "emb = OpenAIEmbeddings(model = 'text-embedding-3-small')\n",
        "# 3) 임베딩된 문장을 Chroma DB에 저장! Vector DB에 저장\n",
        "db  = Chroma.from_documents(chunks, emb)\n",
        "# 향후 db에 있는 내용을 retriever로 가져오기만하면 되는 상황\n",
        "\n",
        "# RAG기능 테스트! 간단 UI\n",
        "# 4) streamlit UI 적용\n",
        "st.write(\"RAG기반 문서 질의\")\n",
        "q = st.text_input(\"질문을 입력하세요\", placeholder = \"예: VIP환불정책을 알려주세요\")\n",
        "\n",
        "if q:\n",
        "    # 1) 위에서 3가지 유사 문장을 가져온다.\n",
        "    hits = db.similarity_search(q, k=3)\n",
        "    # 2) 그 문장을 하나의 문장으로 병합한다.\n",
        "    context = \"\\n\\n\".join(d.page_content for d in hits)\n",
        "    # 3) 응답을 할 수 있게 구현한다.\n",
        "    answer = chain.invoke({'context' : context, \"q\": q})\n",
        "\n",
        "    st.markdown(f\"답변 : {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXsIfgZKjv0F",
        "outputId": "49cce974-aa71-4fa5-e645-3f8434682d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3단계 - 키워드 기반 자동 라우팅 - 키워드에 따라 일반 대화 및 RAG 답변을 하는 챗봇 구현\n",
        "%%writefile app.py\n",
        "import os, uuid\n",
        "import streamlit as st\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings # 2단계 추가\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_community.document_loaders import TextLoader # 2단계 추가\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter # 2단계 추가\n",
        "from langchain_community.vectorstores import Chroma # 2단계 추가\n",
        "\n",
        "# 2단계에 필요한 라이브러리 추가\n",
        "\n",
        "# 기본 메인 화면 설정 페이지 타이틀 / 화면 타이틀\n",
        "st.set_page_config(page_title = \"RAG 챗봇 만들기 Step 3\")\n",
        "st.title(\"자동 라우팅 RAG 챗봇\")\n",
        "\n",
        "# 세션 초기화\n",
        "# 임시 메모리 -> 새로고침 전까지 유지되는 메모리 - 대화 내용이 없으면 빈 리스트!\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state[\"messages\"] = []\n",
        "\n",
        "# LLM 구성\n",
        "# 1. AI의 목적 및 프롬프트\n",
        "# 2. 사용자의 대화를 전달받는 q {q}\n",
        "# 3. 답변을 비워놔서 AI답변을 기다린다.\n",
        "llm = ChatOpenAI(model = 'gpt-4o-mini')\n",
        "\n",
        "\n",
        "# RAG 대화 전용 (이름 변경)\n",
        "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "너는 친절한 한국어 비서야.\n",
        "아래 문서 내용을 참고해 질문에 답하세요.\n",
        "문서   : {context}\n",
        "사용자 : {q}\n",
        "답변  :\n",
        "\"\"\")\n",
        "rag_chain = rag_prompt | llm | StrOutputParser()\n",
        "\n",
        "# 일반 대화 전용 LCEL 구축 (일반 대화 - 추가)\n",
        "base_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "너는 친절한 한국어 비서야\n",
        "사용자 : {q}\n",
        "답변  :\n",
        "\"\"\"\n",
        ")\n",
        "base_chain = base_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "# RAG관련 문서 추가 - 수업에서는 하단에 있는 4줄 샘플로 진행\n",
        "# 3) 문서→임베딩→DB (2교시 신규 추가)\n",
        "os.makedirs(\"data\", exist_ok=True)                                     # [추가]\n",
        "doc_path = \"data/정책.md\"                                              # [추가]\n",
        "if not os.path.exists(doc_path):                                       # [추가: 샘플 생성]\n",
        "    with open(doc_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\n",
        "            \"- 디지털 상품은 원칙적으로 환불 불가 (결제 오류·중복 결제 예외)\\n\"\n",
        "            \"- 실물 상품은 수령 7일 이내 교환/환불 가능(미개봉)\\n\"\n",
        "            \"- VIP(GOLD) 고객은 1회 예외 환불 가능 (관리자 승인)\\n\"\n",
        "            \"- 배송 지연 3영업일 초과 시 사과 쿠폰 지급(내부 승인)\\n\"\n",
        "        )\n",
        "\n",
        "# 상단에 4줄 문장을 읽을 준비 완료\n",
        "loader = TextLoader(doc_path, encoding = 'utf-8')\n",
        "docs = loader.load()\n",
        "\n",
        "# RAG관련 문서 정제\n",
        "# 1) 문장을 청크(chunk)해서 나누고, 겹치게 구성\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size = 800, chunk_overlap = 100)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "# 2) 임베딩 - 문장을 숫자화\n",
        "emb = OpenAIEmbeddings(model = 'text-embedding-3-small')\n",
        "# 3) 임베딩된 문장을 Chroma DB에 저장! Vector DB에 저장\n",
        "db  = Chroma.from_documents(chunks, emb)\n",
        "# 향후 db에 있는 내용을 retriever로 가져오기만하면 되는 상황\n",
        "\n",
        "# 자동 라우팅 규칙\n",
        "# 사용자 질문에 해당 키워드가 있으면 RAG로 - 그렇지 않으면 일반 대화로\n",
        "RAG_KEYWORDS = [\n",
        "    \"환불\", \"교환\", \"반품\", \"정책\", \"내규\", \"규정\", \"쿠폰\", \"배송\", \"VIP\", \"GOLD\", \"SILVER\", \"고객센터\", \"상담\"\n",
        "]\n",
        "\n",
        "def route_to_rag(q):\n",
        "    ql = q.lower()\n",
        "    return any(kw.lower() in ql for kw in RAG_KEYWORDS)\n",
        "\n",
        "# 위에 관련된 키워드가 등장했을때 RAG 챗봇이 등장!\n",
        "def ask_rag(q):\n",
        "    hits = db.similarity_search(q, k=3)\n",
        "    context = \"\\n\\n\".join(d.page_content for d in hits)\n",
        "    return rag_chain.invoke({'context' : context, \"q\": q})\n",
        "\n",
        "\n",
        "# Streamlit 가장 중요한 요소 중 1개 chat_message - 스트림릿의 채팅 UI요소 뼈대\n",
        "# session_state가 가지고 있는 메모리 -> 채팅UI 출력물을 뿌려줘야 하는데\n",
        "# role 사람인지 챗봇인지 구별 -> content를 채팅 UI에 출력!\n",
        "for m in st.session_state[\"messages\"]:\n",
        "    with st.chat_message(m['role']):\n",
        "        st.markdown(m[\"content\"])\n",
        "\n",
        "# 알맹이 사용자 입력 -> LLM 응답 -> 렌더링(실시간 대화)\n",
        "q = st.chat_input(\"질문을 입력해주세요\") # input 설정\n",
        "if q: #q가 질문이 입력이 되었다면\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(q) # q출력\n",
        "    st.session_state['messages'].append({'role' : 'user', 'content' : q})\n",
        "\n",
        "    # 조건부 라우팅 로직\n",
        "    use_rag = route_to_rag(q) # 추가\n",
        "\n",
        "    with st.chat_message('assistant'):\n",
        "        if use_rag:\n",
        "            ans = ask_rag(q) # 변경 - ask_rag를 활용한 답변\n",
        "            st.markdown(ans)\n",
        "            st.caption(\"문서기반 RAG로 답변 되었습니다.\")\n",
        "        else:\n",
        "            ans = base_chain.invoke({\"q\": q})\n",
        "            st.markdown(ans)\n",
        "            st.caption(\"일반 LLM으로 답변 되었습니다.\")\n",
        "    st.session_state['messages'].append({'role' : 'assistant', 'content': ans})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcWy85SVw6nm",
        "outputId": "52f17d86-e941-4c47-8ffd-2eff863928fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4단계 - 근거 표시 출처, 하이라이트 기능 추가\n",
        "%%writefile app.py\n",
        "import os, uuid\n",
        "import streamlit as st\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings # 2단계 추가\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_community.document_loaders import TextLoader # 2단계 추가\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter # 2단계 추가\n",
        "from langchain_community.vectorstores import Chroma # 2단계 추가\n",
        "\n",
        "# 2단계에 필요한 라이브러리 추가\n",
        "\n",
        "# 기본 메인 화면 설정 페이지 타이틀 / 화면 타이틀\n",
        "st.set_page_config(page_title = \"RAG 챗봇 만들기 Step 4\")\n",
        "st.title(\"근거 표시 및 출처 기능 추가\")\n",
        "\n",
        "# 세션 초기화\n",
        "# 임시 메모리 -> 새로고침 전까지 유지되는 메모리 - 대화 내용이 없으면 빈 리스트!\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state[\"messages\"] = []\n",
        "\n",
        "# LLM 구성\n",
        "# 1. AI의 목적 및 프롬프트\n",
        "# 2. 사용자의 대화를 전달받는 q {q}\n",
        "# 3. 답변을 비워놔서 AI답변을 기다린다.\n",
        "llm = ChatOpenAI(model = 'gpt-4o-mini')\n",
        "\n",
        "\n",
        "# RAG 대화 전용 (이름 변경)\n",
        "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "너는 친절한 한국어 비서야.\n",
        "아래 문서 내용을 참고해 질문에 답하세요.\n",
        "문서   : {context}\n",
        "사용자 : {q}\n",
        "답변  :\n",
        "\"\"\")\n",
        "rag_chain = rag_prompt | llm | StrOutputParser()\n",
        "\n",
        "# 일반 대화 전용 LCEL 구축 (일반 대화 - 추가)\n",
        "base_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "너는 친절한 한국어 비서야\n",
        "사용자 : {q}\n",
        "답변  :\n",
        "\"\"\"\n",
        ")\n",
        "base_chain = base_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "# RAG관련 문서 추가 - 수업에서는 하단에 있는 4줄 샘플로 진행\n",
        "# 3) 문서→임베딩→DB (2교시 신규 추가)\n",
        "os.makedirs(\"data\", exist_ok=True)                                     # [추가]\n",
        "doc_path = \"data/정책.md\"                                              # [추가]\n",
        "if not os.path.exists(doc_path):                                       # [추가: 샘플 생성]\n",
        "    with open(doc_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\n",
        "            \"- 디지털 상품은 원칙적으로 환불 불가 (결제 오류·중복 결제 예외)\\n\"\n",
        "            \"- 실물 상품은 수령 7일 이내 교환/환불 가능(미개봉)\\n\"\n",
        "            \"- VIP(GOLD) 고객은 1회 예외 환불 가능 (관리자 승인)\\n\"\n",
        "            \"- 배송 지연 3영업일 초과 시 사과 쿠폰 지급(내부 승인)\\n\"\n",
        "        )\n",
        "\n",
        "# 상단에 4줄 문장을 읽을 준비 완료\n",
        "loader = TextLoader(doc_path, encoding = 'utf-8')\n",
        "docs = loader.load()\n",
        "\n",
        "# RAG관련 문서 정제\n",
        "# 1) 문장을 청크(chunk)해서 나누고, 겹치게 구성\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size = 800, chunk_overlap = 100)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "# 2) 임베딩 - 문장을 숫자화\n",
        "emb = OpenAIEmbeddings(model = 'text-embedding-3-small')\n",
        "# 3) 임베딩된 문장을 Chroma DB에 저장! Vector DB에 저장\n",
        "db  = Chroma.from_documents(chunks, emb)\n",
        "# 향후 db에 있는 내용을 retriever로 가져오기만하면 되는 상황\n",
        "\n",
        "# 자동 라우팅 규칙\n",
        "# 사용자 질문에 해당 키워드가 있으면 RAG로 - 그렇지 않으면 일반 대화로\n",
        "RAG_KEYWORDS = [\n",
        "    \"환불\", \"교환\", \"반품\", \"정책\", \"내규\", \"규정\", \"쿠폰\", \"배송\", \"VIP\", \"GOLD\", \"SILVER\", \"고객센터\", \"상담\"\n",
        "]\n",
        "\n",
        "def route_to_rag(q):\n",
        "    ql = q.lower()\n",
        "    return any(kw.lower() in ql for kw in RAG_KEYWORDS)\n",
        "\n",
        "# 근거를 표시하는 RAG 함수 구현~! (유사도 점수 + 근거 문장 표시)\n",
        "def ask_rag_with_source(q):\n",
        "    results = db.similarity_search_with_relevance_scores(q, k=3)\n",
        "    if not results:\n",
        "        return \"관련 문서를 찾지 못했습니다.\"\n",
        "    # 컨텍스트를 생성 -> 아규먼트 _추가 출처 result로 변경\n",
        "    context = \"\\n\\n\".join(d.page_content for d, _ in results)\n",
        "\n",
        "    # 답변을 생성을 기반으로 출처가 어디에서 나왔는지 확인하는 절차를 추가!\n",
        "    # 1) 답변 생성\n",
        "    answer = rag_chain.invoke({'context' : context, \"q\": q})\n",
        "    # 2) 근거 목록 생성 (문단 일부 + 점수) - 핵심\n",
        "    # source가 존재하면 -> 정책문서! / 유사도 점수를 기반으로 얼마나 연관되어있는지 / 실제 문서 근거\n",
        "    sources = [\n",
        "        f\" {d.metadata.get('source',\"정책문서\")} 유사도 {score:.2f} \\n {d.page_content[:120]}\" for d, score in results\n",
        "    ]\n",
        "\n",
        "    return answer, sources\n",
        "\n",
        "\n",
        "# Streamlit 가장 중요한 요소 중 1개 chat_message - 스트림릿의 채팅 UI요소 뼈대\n",
        "# session_state가 가지고 있는 메모리 -> 채팅UI 출력물을 뿌려줘야 하는데\n",
        "# role 사람인지 챗봇인지 구별 -> content를 채팅 UI에 출력!\n",
        "for m in st.session_state[\"messages\"]:\n",
        "    with st.chat_message(m['role']):\n",
        "        st.markdown(m[\"content\"])\n",
        "\n",
        "# 알맹이 사용자 입력 -> LLM 응답 -> 렌더링(실시간 대화)\n",
        "q = st.chat_input(\"질문을 입력해주세요\") # input 설정\n",
        "if q: #q가 질문이 입력이 되었다면\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(q) # q출력\n",
        "    st.session_state['messages'].append({'role' : 'user', 'content' : q})\n",
        "\n",
        "    # 조건부 라우팅 로직\n",
        "    use_rag = route_to_rag(q) # 추가\n",
        "\n",
        "    with st.chat_message('assistant'):\n",
        "        if use_rag:\n",
        "            ans, sources = ask_rag_with_source(q) # 변경 - ask_rag_with_source를 실행해서 출처까지 답변\n",
        "            st.markdown(ans)\n",
        "            st.caption(\"문서기반 RAG로 답변 되었습니다.\")\n",
        "\n",
        "            if sources:\n",
        "                st.markdown(\"참고근거\")\n",
        "                for s in sources:\n",
        "                    st.markdown(s)\n",
        "\n",
        "        else:\n",
        "            ans = base_chain.invoke({\"q\": q})\n",
        "            st.markdown(ans)\n",
        "            st.caption(\"일반 LLM으로 답변 되었습니다.\")\n",
        "    st.session_state['messages'].append({'role' : 'assistant', 'content': ans})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDS8g5UcMP0u",
        "outputId": "7a1da962-ad74-4256-d61c-734fbdf31c2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5단계 - 고객 질의 RAG 챗봇 구현\n",
        "\n",
        "%%writefile app.py\n",
        "import os, uuid\n",
        "import streamlit as st\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings # 2단계 추가\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_community.document_loaders import TextLoader # 2단계 추가\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter # 2단계 추가\n",
        "from langchain_community.vectorstores import Chroma # 2단계 추가\n",
        "import pandas as pd  # 5단계 추가\n",
        "from langchain_experimental.agents.agent_toolkits.pandas.base import create_pandas_dataframe_agent # 5단계 추가\n",
        "\n",
        "# 2단계에 필요한 라이브러리 추가\n",
        "\n",
        "# 기본 메인 화면 설정 페이지 타이틀 / 화면 타이틀\n",
        "st.set_page_config(page_title = \"RAG 챗봇 만들기 Step 5\")\n",
        "st.title(\"고객 DB기반 개인화 RAG 챗봇 구현\")\n",
        "\n",
        "# 세션 초기화\n",
        "# 임시 메모리 -> 새로고침 전까지 유지되는 메모리 - 대화 내용이 없으면 빈 리스트!\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state[\"messages\"] = []\n",
        "\n",
        "# LLM 구성\n",
        "# 1. AI의 목적 및 프롬프트\n",
        "# 2. 사용자의 대화를 전달받는 q {q}\n",
        "# 3. 답변을 비워놔서 AI답변을 기다린다.\n",
        "llm = ChatOpenAI(model = 'gpt-4o-mini')\n",
        "\n",
        "# 고객 CSV 파일 로드\n",
        "\n",
        "# 경로 지정\n",
        "os.makedirs(\"data\", exist_ok = True)\n",
        "csv_path = \"data/customer_db.csv\"\n",
        "if not os.path.exists(csv_path):\n",
        "    df_sample = pd.DataFrame([\n",
        "        {\"name\" : \"김민수\", 'tier' : \"GOLD\", \"purchase_count\" : 23, \"last_order\" : \"2025-10-20\"},\n",
        "        {\"name\" : \"이지은\", 'tier' : \"SILVER\", \"purchase_count\" : 5, \"last_order\" : \"2025-10-23\"},\n",
        "        {\"name\" : \"박철수\", 'tier' : \"VIP\", \"purchase_count\" : 40, \"last_order\" : \"2025-10-25\"},\n",
        "    ])\n",
        "    df_sample.to_csv(csv_path, index=False, encoding ='utf-8')\n",
        "\n",
        "df = pd.read_csv(csv_path, encoding = 'utf-8')\n",
        "st.write(\"고객 DB 로드 완료\", len(df), \"명\")\n",
        "\n",
        "# 판다스 에이전트 생성\n",
        "# df라는 데이터베이스 파일을 llm으로 읽어서 query기반으로 수행할 수 있는 에이전트 구현\n",
        "agent = create_pandas_dataframe_agent(\n",
        "    llm,\n",
        "    df,\n",
        "    verbose = False,\n",
        "    allow_dangerous_code = True,\n",
        "    max_iterations = 10\n",
        ")\n",
        "\n",
        "# RAG 대화 전용 (이름 변경) - 고객정보 추가!\n",
        "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "너는 친절한 한국어 비서야.\n",
        "아래 문서 내용을 참고해 질문에 답하세요.\n",
        "고객정보 : {customer}\n",
        "문서   : {context}\n",
        "사용자 : {q}\n",
        "답변  :\n",
        "\"\"\")\n",
        "rag_chain = rag_prompt | llm | StrOutputParser()\n",
        "\n",
        "# 일반 대화 전용 LCEL 구축 (일반 대화 - 추가)\n",
        "base_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "너는 친절한 한국어 비서야\n",
        "사용자 : {q}\n",
        "답변  :\n",
        "\"\"\"\n",
        ")\n",
        "base_chain = base_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "# RAG관련 문서 추가 - 수업에서는 하단에 있는 4줄 샘플로 진행\n",
        "# 3) 문서→임베딩→DB (2교시 신규 추가)\n",
        "os.makedirs(\"data\", exist_ok=True)                                     # [추가]\n",
        "doc_path = \"data/정책.md\"                                              # [추가]\n",
        "if not os.path.exists(doc_path):                                       # [추가: 샘플 생성]\n",
        "    with open(doc_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\n",
        "            \"- 디지털 상품은 원칙적으로 환불 불가 (결제 오류·중복 결제 예외)\\n\"\n",
        "            \"- 실물 상품은 수령 7일 이내 교환/환불 가능(미개봉)\\n\"\n",
        "            \"- VIP(GOLD) 고객은 1회 예외 환불 가능 (관리자 승인)\\n\"\n",
        "            \"- 배송 지연 3영업일 초과 시 사과 쿠폰 지급(내부 승인)\\n\"\n",
        "        )\n",
        "\n",
        "# 상단에 4줄 문장을 읽을 준비 완료\n",
        "loader = TextLoader(doc_path, encoding = 'utf-8')\n",
        "docs = loader.load()\n",
        "\n",
        "# RAG관련 문서 정제\n",
        "# 1) 문장을 청크(chunk)해서 나누고, 겹치게 구성\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size = 800, chunk_overlap = 100)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "# 2) 임베딩 - 문장을 숫자화\n",
        "emb = OpenAIEmbeddings(model = 'text-embedding-3-small')\n",
        "# 3) 임베딩된 문장을 Chroma DB에 저장! Vector DB에 저장\n",
        "db  = Chroma.from_documents(chunks, emb)\n",
        "# 향후 db에 있는 내용을 retriever로 가져오기만하면 되는 상황\n",
        "\n",
        "# 자동 라우팅 규칙\n",
        "# 사용자 질문에 해당 키워드가 있으면 RAG로 - 그렇지 않으면 일반 대화로\n",
        "RAG_KEYWORDS = [\n",
        "    \"환불\", \"교환\", \"반품\", \"정책\", \"내규\", \"규정\", \"쿠폰\", \"배송\", \"VIP\", \"GOLD\", \"SILVER\", \"고객센터\", \"상담\", \"등급\"\n",
        "]\n",
        "\n",
        "def route_to_rag(q):\n",
        "    ql = q.lower()\n",
        "    return any(kw.lower() in ql for kw in RAG_KEYWORDS)\n",
        "\n",
        "# 근거를 표시하는 RAG 함수 구현~! (유사도 점수 + 근거 문장 표시)\n",
        "def personalized_rag(q):\n",
        "    # 1. 고객 이름 추출 시도\n",
        "    import re\n",
        "    name = [n for n in df['name'] if n in q]\n",
        "    if not name:\n",
        "      return \"고객 이름이 인식되지 않습니다. 예) 김민수 고객 환불 가능해?\", []\n",
        "    name = name[0]\n",
        "\n",
        "    res = agent.invoke(f\"{name}에 대한 고객 정보를 요약해줘\")\n",
        "    # invoke결과가 문자열 / 딕셔너리 형태로도 나올수 있어서 문자열이면 그냥 출력 -> 딕셔너리 일 경우에는 요약문자열을 출력해라\n",
        "    customer_info = res if isinstance(res, str) else res.get(\"output\", str(res))\n",
        "\n",
        "    results = db.similarity_search_with_relevance_scores(q, k=3)\n",
        "    if not results:\n",
        "        return \"관련 문서를 찾지 못했습니다.\", []\n",
        "    # 컨텍스트를 생성 -> 아규먼트 _추가 출처 result로 변경\n",
        "    context = \"\\n\\n\".join(d.page_content for d, _ in results)\n",
        "\n",
        "    # 답변을 생성을 기반으로 출처가 어디에서 나왔는지 확인하는 절차를 추가!\n",
        "    # 1) 답변 생성\n",
        "    answer = rag_chain.invoke({'customer': customer_info, 'context' : context, \"q\": q})\n",
        "    # 2) 근거 목록 생성 (문단 일부 + 점수) - 핵심\n",
        "    # source가 존재하면 -> 정책문서! / 유사도 점수를 기반으로 얼마나 연관되어있는지 / 실제 문서 근거\n",
        "    sources = [\n",
        "        f\" {d.metadata.get('source',\"정책문서\")} 유사도 {score:.2f} \\n {d.page_content[:120]}\" for d, score in results\n",
        "    ]\n",
        "\n",
        "    return answer, sources\n",
        "\n",
        "\n",
        "# Streamlit 가장 중요한 요소 중 1개 chat_message - 스트림릿의 채팅 UI요소 뼈대\n",
        "# session_state가 가지고 있는 메모리 -> 채팅UI 출력물을 뿌려줘야 하는데\n",
        "# role 사람인지 챗봇인지 구별 -> content를 채팅 UI에 출력!\n",
        "for m in st.session_state[\"messages\"]:\n",
        "    with st.chat_message(m['role']):\n",
        "        st.markdown(m[\"content\"])\n",
        "\n",
        "# 알맹이 사용자 입력 -> LLM 응답 -> 렌더링(실시간 대화)\n",
        "q = st.chat_input(\"질문을 입력해주세요\") # input 설정\n",
        "if q: #q가 질문이 입력이 되었다면\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(q) # q출력\n",
        "    st.session_state['messages'].append({'role' : 'user', 'content' : q})\n",
        "\n",
        "    # 조건부 라우팅 로직\n",
        "    use_rag = route_to_rag(q) # 추가\n",
        "\n",
        "    with st.chat_message('assistant'):\n",
        "        if use_rag:\n",
        "            ans, sources = personalized_rag(q) # 변경 - personalized_rag를 실행해서 출처까지 답변\n",
        "            st.markdown(ans)\n",
        "            st.caption(\"DB, 문서기반 RAG로 답변 되었습니다.\")\n",
        "\n",
        "            if sources:\n",
        "                st.markdown(\"참고근거\")\n",
        "                for s in sources:\n",
        "                    st.markdown(s)\n",
        "\n",
        "        else:\n",
        "            ans = base_chain.invoke({\"q\": q})\n",
        "            st.markdown(ans)\n",
        "            st.caption(\"일반 LLM으로 답변 되었습니다.\")\n",
        "    st.session_state['messages'].append({'role' : 'assistant', 'content': ans})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd10bFdfTZ_v",
        "outputId": "4df48f50-8401-4065-929a-2cd127c7d4a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6교시는 메모리 + UI + 속도 최적화\n",
        "\n",
        "%%writefile app.py\n",
        "import os, uuid\n",
        "import streamlit as st\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings # 2단계 추가\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_community.document_loaders import TextLoader # 2단계 추가\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter # 2단계 추가\n",
        "from langchain_community.vectorstores import Chroma # 2단계 추가\n",
        "import pandas as pd  # 5단계 추가\n",
        "from langchain_experimental.agents.agent_toolkits.pandas.base import create_pandas_dataframe_agent # 5단계 추가\n",
        "\n",
        "# 2단계에 필요한 라이브러리 추가\n",
        "\n",
        "# 기본 메인 화면 설정 페이지 타이틀 / 화면 타이틀\n",
        "st.set_page_config(page_title = \"RAG 챗봇 만들기 Step 6\")\n",
        "st.title(\"최종 RAG 챗봇 구현\")\n",
        "\n",
        "# 사이드바(UI) 옵션 추가\n",
        "with st.sidebar:\n",
        "    st.header(\"사이드바 옵션\")\n",
        "    k = st.slider(\"검색 개수 k\", 1, 5, 2, help = \"RAG 검색시 가져올 문서 수\")\n",
        "    show_sources = st.checkbox(\"근거(소스)표시, value = True\")\n",
        "    st.divider()\n",
        "    c1, c2 = st.columns(2)\n",
        "    if c1.button(\"새로운 대화 시작\"):\n",
        "        st.session_state.pop(\"messages\", None) # 메세지도 초기화\n",
        "        st.session_state.pop(\"history\", None)  # 메모리도 초기화\n",
        "        st.rerun() # 새로고침\n",
        "    if c2.button(\"메모리 초기화\"):\n",
        "        st.session_state.pop(\"history\", None) # 메모리만 초기화\n",
        "        st.success(\"메모리가 초기화 되었습니다.\")\n",
        "    st.caption(\"이름/정책 키워드가 없으면 일반 대화로 자동 전환됩니다.\")\n",
        "\n",
        "# 세션 초기화\n",
        "# 임시 메모리 -> 새로고침 전까지 유지되는 메모리 - 대화 내용이 없으면 빈 리스트!\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state[\"messages\"] = []\n",
        "if \"history\" not in st.session_state:\n",
        "    st.session_state[\"history\"] = []\n",
        "\n",
        "# LLM 구성\n",
        "# 1. AI의 목적 및 프롬프트\n",
        "# 2. 사용자의 대화를 전달받는 q {q}\n",
        "# 3. 답변을 비워놔서 AI답변을 기다린다.\n",
        "llm = ChatOpenAI(model = 'gpt-4o-mini')\n",
        "\n",
        "# 고객 CSV 파일 로드\n",
        "\n",
        "# 경로 지정\n",
        "os.makedirs(\"data\", exist_ok = True)\n",
        "csv_path = \"data/customer_db.csv\"\n",
        "if not os.path.exists(csv_path):\n",
        "    df_sample = pd.DataFrame([\n",
        "        {\"name\" : \"김민수\", 'tier' : \"GOLD\", \"purchase_count\" : 23, \"last_order\" : \"2025-10-20\"},\n",
        "        {\"name\" : \"이지은\", 'tier' : \"SILVER\", \"purchase_count\" : 5, \"last_order\" : \"2025-10-23\"},\n",
        "        {\"name\" : \"박철수\", 'tier' : \"VIP\", \"purchase_count\" : 40, \"last_order\" : \"2025-10-25\"},\n",
        "    ])\n",
        "    df_sample.to_csv(csv_path, index=False, encoding ='utf-8')\n",
        "\n",
        "df = pd.read_csv(csv_path, encoding = 'utf-8')\n",
        "st.write(\"고객 DB 로드 완료\", len(df), \"명\")\n",
        "\n",
        "# 판다스 에이전트 생성\n",
        "# df라는 데이터베이스 파일을 llm으로 읽어서 query기반으로 수행할 수 있는 에이전트 구현\n",
        "agent = create_pandas_dataframe_agent(\n",
        "    llm,\n",
        "    df,\n",
        "    verbose = False,\n",
        "    allow_dangerous_code = True,\n",
        "    max_iterations = 10\n",
        ")\n",
        "\n",
        "\n",
        "# 최근 대화 n턴을 문자열로 만드는 기능\n",
        "def get_history_text(n_turns = 6):\n",
        "    hist = st.session_state['history'][-n_turns:] # 최근 정보부터 가져온다.\n",
        "    lines = []\n",
        "    for role, content in hist:\n",
        "        prefix = \"사용자\" if role == \"user\" else \"어시스턴트\"\n",
        "        lines.append(f\"{prefix}: {content}\")\n",
        "    return \"\\n\".join(lines) if lines else \"대화이력 없음\"\n",
        "\n",
        "\n",
        "# RAG 대화 전용 (이름 변경) - 고객정보 추가!\n",
        "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "너는 친절한 한국어 비서야.\n",
        "아래 문서 내용을 참고해 질문에 답하세요.\n",
        "최근대화 : {history}\n",
        "고객정보 : {customer}\n",
        "문서   : {context}\n",
        "사용자 : {q}\n",
        "답변  :\n",
        "\"\"\")\n",
        "rag_chain = rag_prompt | llm | StrOutputParser()\n",
        "\n",
        "# 일반 대화 전용 LCEL 구축 (일반 대화 - 추가)\n",
        "base_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "너는 친절한 한국어 비서야\n",
        "최근대화 : {history}\n",
        "사용자 : {q}\n",
        "답변  :\n",
        "\"\"\"\n",
        ")\n",
        "base_chain = base_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "# RAG관련 문서 추가 - 수업에서는 하단에 있는 4줄 샘플로 진행\n",
        "# 3) 문서→임베딩→DB (2교시 신규 추가)\n",
        "os.makedirs(\"data\", exist_ok=True)                                     # [추가]\n",
        "doc_path = \"data/정책.md\"                                              # [추가]\n",
        "if not os.path.exists(doc_path):                                       # [추가: 샘플 생성]\n",
        "    with open(doc_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\n",
        "            \"- 디지털 상품은 원칙적으로 환불 불가 (결제 오류·중복 결제 예외)\\n\"\n",
        "            \"- 실물 상품은 수령 7일 이내 교환/환불 가능(미개봉)\\n\"\n",
        "            \"- VIP(GOLD) 고객은 1회 예외 환불 가능 (관리자 승인)\\n\"\n",
        "            \"- 배송 지연 3영업일 초과 시 사과 쿠폰 지급(내부 승인)\\n\"\n",
        "        )\n",
        "\n",
        "# 상단에 4줄 문장을 읽을 준비 완료\n",
        "loader = TextLoader(doc_path, encoding = 'utf-8')\n",
        "docs = loader.load()\n",
        "\n",
        "# RAG관련 문서 정제\n",
        "# 1) 문장을 청크(chunk)해서 나누고, 겹치게 구성\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size = 800, chunk_overlap = 100)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "# 2) 임베딩 - 문장을 숫자화\n",
        "emb = OpenAIEmbeddings(model = 'text-embedding-3-small')\n",
        "# 3) 임베딩된 문장을 Chroma DB에 저장! Vector DB에 저장\n",
        "# -> 수정된 Chroma DB를 통해 데이터베이스를 저장해서 그 DB사용하는 방식으로 속도를 개선한다.\n",
        "os.makedirs(\"db\", exist_ok=True)\n",
        "db  = Chroma.from_documents(chunks, emb, persist_directory=\"db\")\n",
        "db.persist()\n",
        "# 향후 db에 있는 내용을 retriever로 가져오기만하면 되는 상황\n",
        "\n",
        "# 자동 라우팅 규칙\n",
        "# 사용자 질문에 해당 키워드가 있으면 RAG로 - 그렇지 않으면 일반 대화로\n",
        "RAG_KEYWORDS = [\n",
        "    \"환불\", \"교환\", \"반품\", \"정책\", \"내규\", \"규정\", \"쿠폰\", \"배송\", \"VIP\", \"GOLD\", \"SILVER\", \"고객센터\", \"상담\", \"등급\"\n",
        "]\n",
        "\n",
        "def route_to_rag(q):\n",
        "    ql = q.lower()\n",
        "    return any(kw.lower() in ql for kw in RAG_KEYWORDS)\n",
        "\n",
        "# 근거를 표시하는 RAG 함수 구현~! (유사도 점수 + 근거 문장 표시)\n",
        "def personalized_rag(q, k_override = None):\n",
        "    # 1. 고객 이름 추출 시도\n",
        "    import re\n",
        "    name = [n for n in df['name'] if n in q]\n",
        "    if not name:\n",
        "      return \"고객 이름이 인식되지 않습니다. 예) 김민수 고객 환불 가능해?\", []\n",
        "    name = name[0]\n",
        "\n",
        "    res = agent.invoke(f\"{name}에 대한 고객 정보를 요약해줘\")\n",
        "    # invoke결과가 문자열 / 딕셔너리 형태로도 나올수 있어서 문자열이면 그냥 출력 -> 딕셔너리 일 경우에는 요약문자열을 출력해라\n",
        "    customer_info = res if isinstance(res, str) else res.get(\"output\", str(res))\n",
        "\n",
        "    # 슬라이드에 있는 k조절 기능으로 인해 따로 기능을 추가~!\n",
        "    kk = k_override if k_override is not None else k\n",
        "    results = db.similarity_search_with_relevance_scores(q, k=kk)\n",
        "\n",
        "    if not results:\n",
        "        return \"관련 문서를 찾지 못했습니다.\", []\n",
        "    # 컨텍스트를 생성 -> 아규먼트 _추가 출처 result로 변경\n",
        "    context = \"\\n\\n\".join(d.page_content for d, _ in results)\n",
        "\n",
        "    history_text = get_history_text()\n",
        "\n",
        "    # 답변을 생성을 기반으로 출처가 어디에서 나왔는지 확인하는 절차를 추가!\n",
        "    # 1) 답변 생성\n",
        "    answer = rag_chain.invoke({'history':history_text, 'customer': customer_info, 'context' : context, \"q\": q})\n",
        "    # 2) 근거 목록 생성 (문단 일부 + 점수) - 핵심\n",
        "    # source가 존재하면 -> 정책문서! / 유사도 점수를 기반으로 얼마나 연관되어있는지 / 실제 문서 근거\n",
        "    sources = [\n",
        "        f\" {d.metadata.get('source',\"정책문서\")} 유사도 {score:.2f} \\n {d.page_content[:120]}\" for d, score in results\n",
        "    ]\n",
        "\n",
        "    return answer, sources\n",
        "\n",
        "\n",
        "# Streamlit 가장 중요한 요소 중 1개 chat_message - 스트림릿의 채팅 UI요소 뼈대\n",
        "# session_state가 가지고 있는 메모리 -> 채팅UI 출력물을 뿌려줘야 하는데\n",
        "# role 사람인지 챗봇인지 구별 -> content를 채팅 UI에 출력!\n",
        "for m in st.session_state[\"messages\"]:\n",
        "    with st.chat_message(m['role']):\n",
        "        st.markdown(m[\"content\"])\n",
        "\n",
        "# 알맹이 사용자 입력 -> LLM 응답 -> 렌더링(실시간 대화)\n",
        "q = st.chat_input(\"질문을 입력해주세요\") # input 설정\n",
        "if q: #q가 질문이 입력이 되었다면\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(q) # q출력\n",
        "    st.session_state['messages'].append({'role' : 'user', 'content' : q})\n",
        "    # 대화이력에 대한 히스토리 추가 (6교시 부분)\n",
        "    st.session_state['history'].append(('user', 'q'))\n",
        "\n",
        "    # 조건부 라우팅 로직\n",
        "    use_rag = route_to_rag(q) # 추가\n",
        "\n",
        "    with st.chat_message('assistant'):\n",
        "        if use_rag:\n",
        "            ans, sources = personalized_rag(q, k_override = k) # 변경 - personalized_rag를 실행해서 출처까지 답변\n",
        "            st.markdown(ans)\n",
        "            st.caption(\"DB, 문서기반 RAG로 답변 되었습니다.\")\n",
        "\n",
        "            if sources:\n",
        "                st.markdown(\"참고근거\")\n",
        "                for s in sources:\n",
        "                    st.markdown(s)\n",
        "\n",
        "        else:\n",
        "            history_text = get_history_text()\n",
        "            ans = base_chain.invoke({\"history\" : history_text, \"q\": q})\n",
        "            st.markdown(ans)\n",
        "            st.caption(\"일반 LLM으로 답변 되었습니다.\")\n",
        "    st.session_state['messages'].append({'role' : 'assistant', 'content': ans})\n",
        "    st.session_state['history'].append(('assistant', ans))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw0WGDyVn5wR",
        "outputId": "efb871e7-2665-4a8b-ab40-c18bf71dfd2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 서버를 자동 종료\n",
        "try: ngrok.kill()\n",
        "except: pass\n",
        "\n",
        "os.system(\"pkill -f streamlit >/dev/null 2>&1 || true\")\n",
        "os.system(\"streamlit run app.py --server.address 0.0.0.0 --server.port 8501 &>/dev/null &\")\n",
        "print(\"🌐\", ngrok.connect(addr=\"http://127.0.0.1:8501\", bind_tls=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yln1XhBSZsH4",
        "outputId": "181b5cea-2e67-456f-e227-178ed6bce642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌐 NgrokTunnel: \"https://a3371c0d71bf.ngrok-free.app\" -> \"http://127.0.0.1:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eKMfgEswbWYB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
